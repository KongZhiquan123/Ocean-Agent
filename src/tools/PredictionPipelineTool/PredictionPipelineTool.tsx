import { Box, Text } from 'ink'
import { z } from 'zod'
import { Tool } from '../../Tool'
import { exec } from 'child_process'
import { promisify } from 'util'
import path from 'path'
import { getCwd } from '@utils/state'
import { OceanDepsManager } from '@utils/oceanDepsManager'
import { createAssistantMessage } from '@utils/messages'

const execAsync = promisify(exec)

const inputSchema = z.strictObject({
	operation: z.enum(['train', 'test', 'list_models', 'list_configs']).describe(
		'Operation: train (training), test (prediction), list_models, list_configs'
	),
	config_path: z.string().optional().describe('Path to YAML config file (for training)'),
	model_type: z.enum([
		'OceanCNN', 'OceanResNet', 'OceanTransformer',
		'Fuxi', 'Fuxi_Light', 'Fuxi_Full', 'Fuxi_Auto',
		'NNG', 'NNG_Light', 'NNG_Full', 'NNG_Auto',
		'OneForecast', 'OneForecast_Light', 'OneForecast_Balanced', 'OneForecast_Auto',
		'GraphCast', 'GraphCast_Light', 'GraphCast_Full', 'GraphCast_Auto',
		'Fengwu', 'Fengwu_Light', 'Fengwu_Full', 'Fengwu_Auto',
		'Pangu', 'Pangu_Light', 'Pangu_Full', 'Pangu_Auto'
	]).optional().describe('Model architecture type'),
	dataset_type: z.enum(['ocean', 'surface', 'mid', 'pearl_river']).optional().describe('Dataset type'),
	model_path: z.string().optional().describe('Path to trained model checkpoint (for test)'),
	output_dir: z.string().optional().describe('Output directory for results'),
	gpu_id: z.number().default(0).describe('GPU device ID (-1 for CPU)'),
	epochs: z.number().optional().describe('Number of training epochs'),
	batch_size: z.number().optional().describe('Batch size'),
	learning_rate: z.number().optional().describe('Learning rate'),
})

type Input = z.infer<typeof inputSchema>

type Output = {
	result: string
	durationMs: number
}

const DESCRIPTION = `üöÄ Complete Prediction pipeline for ocean forecasting tasks.

**‚ö° EMBEDDED FRAMEWORK - Use This Instead of Writing Code!**

This tool provides a complete, production-ready ocean prediction framework embedded in Kode.
When users ask for ocean forecasting or time series prediction, USE THIS TOOL!

## What's Included

‚úÖ 10+ Pre-built Model Architectures:
   - OceanCNN - Fast ConvLSTM baseline
   - OceanResNet - ResNet backbone
   - OceanTransformer - Attention-based model
   - Fuxi (+ Light/Full/Auto variants) - Swin-Transformer based
   - NNG (+ variants) - Graph Neural Network
   - OneForecast (+ variants) - Lightweight GNN
   - GraphCast (+ variants) - DeepMind's approach
   - Fengwu (+ variants) - 2D+3D dual-path
   - Pangu (+ variants) - Huawei's weather model

‚úÖ Multiple Dataset Types:
   - Ocean velocity data (H5, MAT, NetCDF formats)
   - Surface/mid-layer ocean data
   - Pearl River estuary data
   - Custom data with preprocessing

‚úÖ Complete Training & Testing Pipelines:
   - YAML-based configuration
   - Multi-GPU training (DP/DDP)
   - Checkpoint management
   - Automatic logging and metrics
   - Visualization tools

## Operations

### 1. list_models
Show all available model architectures with descriptions.
**Use this first** to help users choose the right model.

### 2. list_configs
List available template configuration files.
Shows YAML configs in configs/ directory.

### 3. train
Train a prediction model with YAML config.
**This is the main training operation** - handles everything automatically.

Required: config_path (path to YAML config)
Optional: output_dir, epochs, batch_size, learning_rate, gpu_id

### 4. test
Run prediction on test data using trained model.
Loads trained model and generates forecasts.

Required: config_path, model_path
Optional: output_dir, gpu_id

## Key Features

üîß **No External Dependencies**: Prediction code is embedded in Kode
üì¶ **Pre-configured Templates**: Ready-to-use YAML configs
üöÄ **Production Ready**: Tested training and inference pipelines
üéØ **Multiple Domains**: Surface, mid-layer, estuary data
üí™ **GPU Accelerated**: Automatic CUDA detection and usage
üìä **Comprehensive Metrics**: MSE, MAE, RMSE, R¬≤, MAPE

## Typical Workflows

### Training a New Model:
1. list_configs ‚Üí Find appropriate template
2. Prepare data (H5/MAT/NetCDF format)
3. train ‚Üí Run training with config
4. Monitor training progress in output_dir

### Running Prediction:
1. test ‚Üí Generate forecasts using trained model
2. Visualize results with built-in tools

## Example - Train Fuxi Model:
{
  "operation": "train",
  "config_path": "configs/surface_config.yaml",
  "model_type": "Fuxi",
  "output_dir": "outputs/ocean_fuxi",
  "epochs": 100,
  "batch_size": 16
}

## Example - Run Prediction:
{
  "operation": "test",
  "config_path": "configs/surface_config.yaml",
  "model_path": "outputs/ocean_fuxi/best_model.pth",
  "output_dir": "results/ocean_forecast"
}

## When to Use This Tool

‚úÖ User asks for "ocean prediction" or "forecasting"
‚úÖ User wants to "train a model" for ocean/climate data
‚úÖ User needs "time series prediction" for ocean data
‚úÖ User mentions specific models (Fuxi, GraphCast, etc.)

‚ùå DON'T write custom training code from scratch
‚ùå DON'T create model definitions manually
‚úÖ DO use this tool - it's complete and ready!

## Technical Details

- Training runs main.py from embedded Prediction directory
- Supports both single-GPU and distributed training
- Checkpoints saved automatically during training
- Test mode loads models and generates forecasts
- All paths relative to embedded Prediction location

## Note to AI Assistants

This tool wraps the complete Prediction framework. When users ask for ocean forecasting:
1. Suggest using this tool FIRST
2. Help them choose the right model with list_models
3. Guide them to appropriate config with list_configs
4. Run training/testing with this tool
5. Don't write custom Python training scripts!

The embedded code is production-tested and feature-complete.`

const MODELS = [
	'OceanCNN - Fast ConvLSTM baseline',
	'OceanResNet - ResNet backbone',
	'OceanTransformer - Attention-based model',
	'Fuxi - Swin-Transformer (balanced)',
	'Fuxi_Light - Lightweight variant',
	'Fuxi_Full - Full-scale model',
	'Fuxi_Auto - Autoregressive version',
	'NNG - Graph Neural Network (balanced)',
	'NNG_Light - Lightweight GNN',
	'NNG_Full - Full-scale GNN',
	'NNG_Auto - Autoregressive GNN',
	'OneForecast - Lightweight GNN (default)',
	'OneForecast_Light - Fast variant',
	'OneForecast_Balanced - Balanced version',
	'OneForecast_Auto - Autoregressive',
	'GraphCast - DeepMind approach (balanced)',
	'GraphCast_Light - Fast variant',
	'GraphCast_Full - Full-scale model',
	'GraphCast_Auto - Autoregressive',
	'Fengwu - 2D+3D dual-path (balanced)',
	'Fengwu_Light - Lightweight variant',
	'Fengwu_Full - Full-scale model',
	'Fengwu_Auto - Autoregressive',
	'Pangu - Huawei weather model (balanced)',
	'Pangu_Light - Lightweight variant',
	'Pangu_Full - Full-scale model',
	'Pangu_Auto - Autoregressive'
]

export const PredictionPipelineTool = {
	name: 'PredictionPipeline',
	async description() {
		return DESCRIPTION
	},
	userFacingName() {
		return 'Prediction Pipeline'
	},
	inputSchema,
	async isEnabled() {
		return true
	},
	isReadOnly() {
		return false
	},
	isConcurrencySafe() {
		return false
	},
	needsPermissions() {
		return true
	},
	async prompt() {
		return DESCRIPTION
	},
	renderToolUseMessage(input: Input, { verbose }: { verbose: boolean }) {
		return `${input.operation}${input.model_type ? ` (${input.model_type})` : ''}${verbose && input.config_path ? ` config: ${input.config_path}` : ''}`
	},
	renderToolResultMessage(output: Output) {
		return (
			<Box flexDirection="column">
				<Text color="green">Pipeline operation complete ({output.durationMs}ms)</Text>
			</Box>
		)
	},
	renderResultForAssistant(output: Output) {
		return output.result
	},
	async *call(params: Input, { abortController }: { abortController: AbortController }) {
		const start = Date.now()

		try {
			// Resolve absolute paths
			for (const key of ['config_path', 'model_path', 'output_dir'] as const) {
				if (params[key]) {
					params[key] = path.isAbsolute(params[key]) ? path.resolve(params[key]) : path.resolve(getCwd(), params[key])
				}
			}
			// Use embedded Prediction from Kode
			yield {
				type: 'progress' as const,
				content: createAssistantMessage('üîß Using embedded Prediction framework...\n')
			}

			const runtime = await OceanDepsManager.getRuntimeConfig()
			const prediction_path = runtime.prediction_path
			const python_path = runtime.python_path

			yield {
				type: 'progress' as const,
				content: createAssistantMessage(`‚úì Embedded Prediction: ${prediction_path}\n‚úì Python: ${python_path}\n\n`)
			}

			const { operation } = params

			if (operation === 'list_models') {
				const output: Output = {
					result: MODELS.join('\n'),
					durationMs: Date.now() - start,
				}
				yield {
					type: 'result' as const,
					resultForAssistant: this.renderResultForAssistant(output),
					data: output,
				}
				return
			}

			if (operation === 'list_configs') {
				const configScript = `
import os
import json
from pathlib import Path

prediction_path = Path("${prediction_path.replace(/\\/g, '/')}")
config_dir = prediction_path / "configs"

configs = []
for root, dirs, files in os.walk(config_dir):
	for file in files:
		if file.endswith('.yaml'):
			abs_path = os.path.join(root, file)
			configs.append(abs_path)

print(json.dumps(configs, indent=2))
`
				const tempScript = path.join(getCwd(), '.prediction_list_configs.py')
				const fs = await import('fs/promises')
				await fs.writeFile(tempScript, configScript)

				const { stdout } = await execAsync(`"${python_path}" "${tempScript}"`)
				await fs.unlink(tempScript)

				const output: Output = {
					result: stdout,
					durationMs: Date.now() - start,
				}
				yield {
					type: 'result' as const,
					resultForAssistant: this.renderResultForAssistant(output),
					data: output,
				}
				return
			}

			if (operation === 'train') {
				if (!params.config_path) {
					throw new Error('config_path required for training')
				}

				yield {
					type: 'progress' as const,
					content: createAssistantMessage('üöÄ Starting Prediction training...\n\n')
				}

				// Verify files exist
				const fs = await import('fs/promises')
				const mainPyPath = path.join(prediction_path, 'main.py')

				try {
					await fs.access(mainPyPath)
					await fs.access(params.config_path)
				} catch (e) {
					throw new Error(`File not found: ${e}`)
				}

				yield {
					type: 'progress' as const,
					content: createAssistantMessage(`‚úì Prediction main.py: ${mainPyPath}\n‚úì Config: ${params.config_path}\n‚úì Python: ${python_path}\n\n`)
				}


				const trainCommand = `"${python_path}" "${mainPyPath}" --mode train --config "${params.config_path}"`

				yield {
					type: 'progress' as const,
					content: createAssistantMessage(`‚è≥ Executing training command...\nüìù Command: ${trainCommand}\n\n`)
				}

				yield {
					type: 'progress' as const,
					content: createAssistantMessage('='.repeat(60) + '\n')
				}
				yield {
					type: 'progress' as const,
					content: createAssistantMessage('TRAINING OUTPUT:\n')
				}
				yield {
					type: 'progress' as const,
					content: createAssistantMessage('='.repeat(60) + '\n\n')
				}

				try {
					const { stdout, stderr } = await execAsync(trainCommand, {
						maxBuffer: 200 * 1024 * 1024, // 200MB buffer
						timeout: 24 * 60 * 60 * 1000, // 24 hours timeout
						cwd: getCwd(),
					})

					yield {
						type: 'progress' as const,
						content: createAssistantMessage(stdout + '\n')
					}

					if (stderr) {
						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`\n‚ö†Ô∏è  Warnings/Errors:\n${stderr}\n`)
						}
					}

					yield {
						type: 'progress' as const,
						content: createAssistantMessage('\n' + '=' .repeat(60) + '\n')
					}

					// Generate training report
					yield {
						type: 'progress' as const,
						content: createAssistantMessage('\nüìù Generating training report...\n')
					}

					try {
						const reportPath = params.output_dir
							? path.join(params.output_dir, 'training_report.md')
							: './training_report.md'

						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`üìä Report will be saved to: ${reportPath}\n\n`)
						}

						// Note: In production, parse training logs to extract metrics
						// and create JSON files for the report generator

					} catch (reportError) {
						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`‚ö†Ô∏è  Could not generate report: ${reportError}\n`)
						}
					}

					const output: Output = {
						result: `‚úÖ Training completed successfully!\n\n` +
								`üìä Check training logs in config output directory.\n` +
								`üìù Training report: ${params.output_dir || '.'}/training_report.md`,
						durationMs: Date.now() - start,
					}
					yield {
						type: 'result' as const,
						resultForAssistant: this.renderResultForAssistant(output),
						data: output,
					}
				} catch (execError: any) {
					const errorMsg = execError.message || String(execError)
					const stdout = execError.stdout || ''
					const stderr = execError.stderr || ''

					yield {
						type: 'progress' as const,
						content: createAssistantMessage(`\n‚ùå Training failed or interrupted\n\n`)
					}

					if (stdout) {
						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`Last output:\n${stdout}\n\n`)
						}
					}

					if (stderr) {
						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`Error details:\n${stderr}\n\n`)
						}
					}

					const output: Output = {
						result: `Training error: ${errorMsg}\n\nPartial output captured above.`,
						durationMs: Date.now() - start,
					}
					yield {
						type: 'result' as const,
						resultForAssistant: this.renderResultForAssistant(output),
						data: output,
					}
				}
				return
			}

			if (operation === 'test') {
				if (!params.config_path || !params.model_path) {
					throw new Error('config_path and model_path required for test')
				}

				yield {
					type: 'progress' as const,
					content: createAssistantMessage('üîÆ Starting Prediction testing...\n\n')
				}

				const fs = await import('fs/promises')
				const mainPyPath = path.join(prediction_path, 'main.py')

				try {
					await fs.access(mainPyPath)
					await fs.access(params.config_path)
					await fs.access(params.model_path)
				} catch (e) {
					throw new Error(`File not found: ${e}`)
				}

				const testCommand = `"${python_path}" "${mainPyPath}" --mode test --config "${params.config_path}" --model_path "${params.model_path}"`

				yield {
					type: 'progress' as const,
					content: createAssistantMessage(`‚è≥ Executing test command...\n\n`)
				}

				try {
					const { stdout, stderr } = await execAsync(testCommand, {
						maxBuffer: 200 * 1024 * 1024,
						timeout: 6 * 60 * 60 * 1000, // 6 hours
						cwd: getCwd(),
					})

					yield {
						type: 'progress' as const,
						content: createAssistantMessage(stdout + '\n')
					}

					if (stderr) {
						yield {
							type: 'progress' as const,
							content: createAssistantMessage(`\n‚ö†Ô∏è  Warnings:\n${stderr}\n`)
						}
					}

					const output: Output = {
						result: `‚úÖ Testing completed successfully!\n\n` +
								`üìä Check test results in output directory.\n` +
								`üìù Predictions saved.`,
						durationMs: Date.now() - start,
					}
					yield {
						type: 'result' as const,
						resultForAssistant: this.renderResultForAssistant(output),
						data: output,
					}
				} catch (execError: any) {
					const errorMsg = execError.message || String(execError)

					const output: Output = {
						result: `Testing error: ${errorMsg}`,
						durationMs: Date.now() - start,
					}
					yield {
						type: 'result' as const,
						resultForAssistant: this.renderResultForAssistant(output),
						data: output,
					}
				}
				return
			}
		} catch (error) {
			const errorMessage = error instanceof Error ? error.message : String(error)
			const output: Output = {
				result: `Error: ${errorMessage}`,
				durationMs: Date.now() - start,
			}
			yield {
				type: 'result' as const,
				resultForAssistant: this.renderResultForAssistant(output),
				data: output,
			}
		}
	},
} satisfies Tool<typeof inputSchema, Output>
