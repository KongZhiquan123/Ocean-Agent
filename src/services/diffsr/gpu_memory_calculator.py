#!/usr/bin/env python3
"""
GPU æ˜¾å­˜è®¡ç®—å™¨
ç”¨äºéªŒè¯ DiffSR è®­ç»ƒé…ç½®æ˜¯å¦ä¼šå¯¼è‡´æ˜¾å­˜æº¢å‡º
"""

import argparse
import yaml
import subprocess
import re
from pathlib import Path


def get_gpu_memory():
    """è·å– GPU æ˜¾å­˜ä¿¡æ¯ (GB)"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True,
            check=True
        )
        # è·å–ç¬¬ä¸€ä¸ª GPU çš„æ˜¾å­˜ (MB)
        memory_mb = int(result.stdout.strip().split('\n')[0])
        memory_gb = memory_mb / 1024
        return memory_gb
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è·å– GPU ä¿¡æ¯: {e}")
        print("ğŸ’¡ è¯·æ‰‹åŠ¨è¾“å…¥ GPU æ˜¾å­˜å¤§å° (GB):")
        return float(input())


def calculate_activation_memory(batch_size, seq_len, height, width, hidden_dim, num_layers):
    """
    è®¡ç®—æ¿€æ´»å€¼æ˜¾å­˜å ç”¨

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦ (é€šå¸¸ä¸º 1)
        height: è¾“å…¥é«˜åº¦
        width: è¾“å…¥å®½åº¦
        hidden_dim: éšè—å±‚ç»´åº¦
        num_layers: ç½‘ç»œå±‚æ•°

    Returns:
        per_layer_gb: å•å±‚æ¿€æ´»æ˜¾å­˜ (GB)
        total_gb: æ€»æ¿€æ´»æ˜¾å­˜ (GB)
    """
    # æ¯ä¸ªæµ®ç‚¹æ•°å  4 å­—èŠ‚ (FP32)
    bytes_per_float = 4

    # å•å±‚æ¿€æ´»
    per_layer_bytes = batch_size * seq_len * height * width * hidden_dim * bytes_per_float
    per_layer_gb = per_layer_bytes / (1024 ** 3)

    # æ€»æ¿€æ´» (æ‰€æœ‰å±‚)
    total_gb = per_layer_gb * num_layers

    return per_layer_gb, total_gb


def read_model_config(config_path):
    """ä» YAML é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹å‚æ•°"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # æå–å…³é”®å‚æ•°
    model_config = config.get('model', {})
    data_config = config.get('data', {})

    hidden_dim = model_config.get('width', model_config.get('hidden_dim', 256))

    # å°è¯•æ¨æ–­å±‚æ•° (ä¸åŒæ¨¡å‹ä¸åŒ)
    model_name = model_config.get('name', '').lower()
    if 'fno' in model_name:
        num_layers = model_config.get('n_layers', 4)
    elif 'unet' in model_name or 'ddpm' in model_name:
        num_layers = model_config.get('num_res_blocks', 2) * 4  # å‡è®¾4ä¸ªé˜¶æ®µ
    else:
        num_layers = model_config.get('num_layers', 10)

    # è¯»å–æ•°æ®é…ç½®
    train_batch = data_config.get('train_batchsize', 8)
    eval_batch = data_config.get('eval_batchsize', train_batch)
    shape = data_config.get('shape', [128, 128])

    if isinstance(shape, list) and len(shape) >= 2:
        height, width = shape[0], shape[1]
    else:
        height = width = 128

    return {
        'train_batchsize': train_batch,
        'eval_batchsize': eval_batch,
        'height': height,
        'width': width,
        'hidden_dim': hidden_dim,
        'num_layers': num_layers,
        'model_name': model_name,
    }


def verify_memory(config_path=None, batch_size=None, height=None, width=None,
                  hidden_dim=None, num_layers=None, gpu_memory=None):
    """éªŒè¯é…ç½®æ˜¯å¦ä¼šå¯¼è‡´æ˜¾å­˜æº¢å‡º"""

    print("=" * 60)
    print("ğŸ§® GPU æ˜¾å­˜è®¡ç®—å™¨")
    print("=" * 60)
    print()

    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œä»ä¸­è¯»å–å‚æ•°
    if config_path:
        print(f"ğŸ“‚ è¯»å–é…ç½®æ–‡ä»¶: {config_path}")
        params = read_model_config(config_path)
        batch_size = batch_size or params['train_batchsize']
        height = height or params['height']
        width = width or params['width']
        hidden_dim = hidden_dim or params['hidden_dim']
        num_layers = num_layers or params['num_layers']
        print(f"âœ“ æ¨¡å‹: {params['model_name']}")
        print()

    # è·å– GPU æ˜¾å­˜
    if gpu_memory is None:
        print("ğŸ” æ£€æµ‹ GPU æ˜¾å­˜...")
        gpu_memory = get_gpu_memory()

    print(f"âœ“ GPU æ€»æ˜¾å­˜: {gpu_memory:.2f} GB")
    print()

    # è®¡ç®—å¯ç”¨æ˜¾å­˜ (70% å®‰å…¨é˜ˆå€¼)
    available_memory = gpu_memory * 0.7
    print(f"ğŸ’¡ å¯ç”¨æ˜¾å­˜ (70%): {available_memory:.2f} GB")
    print(f"   (é¢„ç•™ 30% ç»™æ¨¡å‹æƒé‡ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨)")
    print()

    # è®¡ç®—æ¿€æ´»æ˜¾å­˜
    seq_len = 1  # è¶…åˆ†ä»»åŠ¡é€šå¸¸ä¸º 1
    per_layer, total = calculate_activation_memory(
        batch_size, seq_len, height, width, hidden_dim, num_layers
    )

    print("ğŸ“Š é…ç½®å‚æ•°:")
    print(f"   - Batch Size: {batch_size}")
    print(f"   - Spatial Resolution: {height} Ã— {width}")
    print(f"   - Hidden Dim: {hidden_dim}")
    print(f"   - Num Layers: {num_layers}")
    print(f"   - Seq Length: {seq_len}")
    print()

    print("ğŸ”¢ æ˜¾å­˜å ç”¨:")
    print(f"   - å•å±‚æ¿€æ´»: {per_layer:.4f} GB")
    print(f"   - æ€»æ¿€æ´»æ˜¾å­˜: {total:.2f} GB")
    print()

    # åˆ¤æ–­æ˜¯å¦å®‰å…¨
    if total < available_memory:
        status = "âœ… å®‰å…¨"
        color_code = "\033[92m"  # ç»¿è‰²
        margin = available_memory - total
        print(f"{color_code}{status}\033[0m")
        print(f"   å‰©ä½™æ˜¾å­˜: {margin:.2f} GB ({margin/available_memory*100:.1f}%)")
        print()
        print("ğŸ’š é…ç½®å®‰å…¨ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
        return True
    else:
        status = "âŒ è¶…é™"
        color_code = "\033[91m"  # çº¢è‰²
        excess = total - available_memory
        print(f"{color_code}{status}\033[0m")
        print(f"   è¶…å‡ºæ˜¾å­˜: {excess:.2f} GB")
        print()
        print("âš ï¸  è­¦å‘Š: é…ç½®å¯èƒ½å¯¼è‡´ OOM (Out of Memory)")
        print()
        print("ğŸ”§ å»ºè®®è°ƒæ•´:")

        # è®¡ç®—æ¨èçš„ batch size
        recommended_batch = int(batch_size * available_memory / total)
        if recommended_batch < 1:
            recommended_batch = 1
        print(f"   1. é™ä½ batch size: {batch_size} â†’ {recommended_batch}")

        # è®¡ç®—æ¨èçš„åˆ†è¾¨ç‡
        scale_factor = (available_memory / total) ** 0.5
        recommended_h = int(height * scale_factor)
        recommended_w = int(width * scale_factor)
        print(f"   2. é™ä½åˆ†è¾¨ç‡: [{height}, {width}] â†’ [{recommended_h}, {recommended_w}]")

        print(f"   3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§ batch")
        print()
        return False


def main():
    parser = argparse.ArgumentParser(description='GPU æ˜¾å­˜è®¡ç®—å™¨')
    parser.add_argument('--config', type=str, help='YAML é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--height', type=int, help='è¾“å…¥é«˜åº¦')
    parser.add_argument('--width', type=int, help='è¾“å…¥å®½åº¦')
    parser.add_argument('--hidden_dim', type=int, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_layers', type=int, help='ç½‘ç»œå±‚æ•°')
    parser.add_argument('--gpu_memory', type=float, help='GPU æ˜¾å­˜ (GB)')

    args = parser.parse_args()

    if not args.config and not all([args.batch, args.height, args.width,
                                     args.hidden_dim, args.num_layers]):
        print("âŒ é”™è¯¯: è¯·æä¾›é…ç½®æ–‡ä»¶ (--config) æˆ–æ‰€æœ‰å‚æ•°")
        print()
        print("ç”¨æ³• 1: ä»é…ç½®æ–‡ä»¶è¯»å–")
        print("  python gpu_memory_calculator.py --config configs/fno.yaml")
        print()
        print("ç”¨æ³• 2: æ‰‹åŠ¨æŒ‡å®šå‚æ•°")
        print("  python gpu_memory_calculator.py --batch 8 --height 128 --width 128 \\")
        print("         --hidden_dim 256 --num_layers 10 --gpu_memory 24")
        return

    success = verify_memory(
        config_path=args.config,
        batch_size=args.batch,
        height=args.height,
        width=args.width,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        gpu_memory=args.gpu_memory
    )

    print("=" * 60)

    # è¿”å›é€€å‡ºç  (0: æˆåŠŸ, 1: å¤±è´¥)
    exit(0 if success else 1)


if __name__ == '__main__':
    main()
