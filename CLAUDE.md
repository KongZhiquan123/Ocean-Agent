# CLAUDE.md

This file provides guidance to some tools and implementation details specific to the Claude agent system.

## Data Preprocessing Policy

**ğŸ”´ CRITICAL: Always Use OceanPreprocessPipeline Tool for Data Preparation**

When users provide raw ocean data (NetCDF files, satellite observations, etc.) that needs preprocessing:

### MANDATORY Rules:
- âœ… **ALWAYS USE**: `OceanPreprocessPipeline` tool for data preprocessing
- âœ… **ALWAYS VALIDATE**: Data quality and convergence MUST be checked
- âŒ **NEVER DO**: Write custom Python/pandas/xarray scripts for preprocessing
- âŒ **NEVER DO**: Use FileWrite + Bash to process NC files manually
- âŒ **NEVER DO**: Skip the validation phase
- âŒ **NEVER DO**: Process data without checking convergence metrics

### Why This Matters:
1. **Data Quality**: Built-in CNN validation ensures data is suitable for training
2. **Consistency**: Standardized preprocessing pipeline across all ocean datasets
3. **Convergence**: Automatic detection of data issues before expensive training
4. **Traceability**: Automatic generation of validation reports and quality metrics
5. **Efficiency**: Production-ready preprocessing engine already built

### Validation is MANDATORY:
The preprocessing pipeline includes **two-phase validation**:

**Phase 1: Statistical Validation (Always runs)**
- Missing value analysis
- Outlier detection
- Data distribution checks
- Temporal/spatial continuity

**Phase 2: CNN Convergence Validation (Default: enabled)**
- Trains a lightweight CNN on the preprocessed data
- Checks if loss converges (indicates data is learnable)
- Provides convergence metrics and quality scores
- **If CNN validation fails, the data is NOT ready for production models**

### When to Use:
- User uploads/provides raw NetCDF files (`.nc` files)
- User mentions data from: JAXA, OSTIA, ERA5, CMEMS, etc.
- User asks to "prepare data", "preprocess data", "clean data"
- Before ANY training pipeline (DiffSR, forecasting, etc.)
- When user provides a directory of ocean observation files

### Required Parameters:
```json
{
  "input_dir": "path/to/raw/nc/files",
  "output_dir": "path/to/output",
  "variable_name": "sst",  // or "temperature", "salinity", etc.
  "file_pattern": "*.nc",
  "use_cnn_validation": true  // ALWAYS true unless user explicitly disables
}
```

### Output Files:
After preprocessing, the tool generates:
1. `preprocessed_{variable}.nc` - Cleaned and merged data file
2. `validation_report.md` - **CRITICAL**: Detailed validation report with convergence analysis
3. `validation_results.json` - Machine-readable quality metrics

**IMPORTANT**: Always read and present the validation_report.md to the user. It contains critical information about data quality and whether the data is ready for training.

### Example Workflow:

**User**: "I have SST data from JAXA in the `raw_data/` folder, prepare it for training"

**Assistant**: Must use OceanPreprocessPipeline tool:
```json
{
  "input_dir": "raw_data",
  "output_dir": "preprocessed_data",
  "variable_name": "sst",
  "file_pattern": "*.nc",
  "use_cnn_validation": true
}
```

**After tool completes**:
1. âœ… Read `validation_report.md`
2. âœ… Present key metrics to user (convergence, quality score)
3. âœ… Recommend next steps based on validation results
4. âŒ Do NOT proceed to training if validation failed

### Implementation Note:
The `OceanPreprocessPipeline` tool is implemented in:
- `src/tools/OceanPreprocessPipelineTool/OceanPreprocessPipelineTool.tsx`
- Backend engine: `src/services/preprocessing/main.py`
- Validation: `src/services/preprocessing/validator.py`

Following this policy ensures all datasets are properly validated before expensive model training, preventing wasted compute on poor-quality data.

---

## Visualization Policy

**ğŸ”´ CRITICAL: Always Use OceanVisualization Tool for Plotting**

When you need to create ANY visualization (charts, plots, maps, graphs, figures):

### MANDATORY Rules:
- âœ… **ALWAYS USE**: `OceanVisualization` tool
- âœ… **ALWAYS GENERATE**: Visualizations BEFORE generating the final report
- âœ… **ALWAYS COLLECT**: All output_path values to pass to report generator
- âŒ **NEVER DO**: Write matplotlib/seaborn/plotly Python scripts
- âŒ **NEVER DO**: Use FileWriteTool + BashTool to create plots manually
- âŒ **NEVER DO**: Create custom plotting functions
- âŒ **NEVER DO**: Skip visualization generation when training/inference completes

### Why This Matters:
1. **Consistency**: All visualizations have uniform styling
2. **Correctness**: Proper file paths, no encoding errors
3. **Efficiency**: Production-ready plotting engine already built
4. **Maintenance**: Centralized visualization logic
5. **Integration**: Seamless integration with report generation system

### Integration with Report Generation System

**The Complete Four-Step Workflow:**

```
Step 1: Training/Inference
   â†“ (generates metrics.json, config.json, training_log.csv, predictions.csv, etc.)

Step 2: Visualization Generation â† USE OceanVisualization TOOL
   â†“ (generates PNG/PDF files, returns output paths)

Step 3: Report Generation (report_generator.py - Python Script Automation)
   â†“ (call with --viz_paths parameter)
   â†“ (script auto-fills VIZ_FILE_LIST and VIZ_IMAGES placeholders)
   â†“ (script preserves AI_FILL placeholders for manual analysis)

Step 4: AI Analysis â† YOU MUST DO THIS
   â†“ (Read the generated report: training_report.md or test_report.md)
   â†“ (View the embedded visualization images)
   â†“ (Analyze visual patterns, trends, and insights)
   â†“ (Fill ALL AI_FILL placeholders with detailed, data-driven analysis)
```

âš ï¸ **CRITICAL**: The workflow does NOT end at Step 3. After the report is generated, you MUST proceed to Step 4 and fill all AI_FILL placeholders.

**Critical Steps:**

1. After training/inference completes, **generate ALL necessary visualizations**
2. **Collect all output_path values** returned by each OceanVisualization call
3. **Pass collected paths** to report_generator.py using `--viz_paths` parameter:
   ```bash
   python report_generator.py train config.json metrics.json output.md \
     --viz_paths "outputs/visualizations/loss_curve.png,outputs/visualizations/psnr_curve.png,outputs/visualizations/sst_map.png"
   ```
4. The report generator will automatically:
   - Replace `VIZ_FILE_LIST` placeholder with bullet list of file paths
   - Replace `VIZ_IMAGES` placeholder with embedded image markdown
   - **Preserve `AI_FILL` placeholders** for you to analyze later
5. **YOU MUST then**:
   - Read the generated report (training_report.md or test_report.md)
   - View the embedded visualization images
   - Analyze the visualizations and data
   - **Fill ALL AI_FILL placeholders** with detailed analysis

**Report Placeholders:**

Training/inference reports contain TWO types of placeholders:

**Type 1: Auto-filled by report_generator.py (No action needed)**

- **Section 4.1 - File List**:
  ```markdown
  <!-- VIZ_FILE_LIST: è„šæœ¬è‡ªåŠ¨å¡«å……ï¼Œåˆ—å‡ºæ‰€æœ‰ç”Ÿæˆçš„å¯è§†åŒ–å›¾ç‰‡è·¯å¾„ -->
  ```

- **Section 4.3 - Image Gallery**:
  ```markdown
  <!-- VIZ_IMAGES: è„šæœ¬è‡ªåŠ¨å¡«å……ï¼Œæ’å…¥æ‰€æœ‰å¯è§†åŒ–å›¾ç‰‡ -->
  ```

**Type 2: AI_FILL placeholders (YOU MUST FILL THESE)**

The generated report contains multiple `AI_FILL` placeholders requiring your analysis:

- **Section 2.2 - Training Curves**:
  - `<!-- AI_FILL: æè¿°è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„ä¸‹é™è¶‹åŠ¿ï¼Œåˆ†ææ”¶æ•›æƒ…å†µ -->`
  - `<!-- AI_FILL: æè¿°å­¦ä¹ ç‡å˜åŒ–ç­–ç•¥åŠå…¶å¯¹è®­ç»ƒçš„å½±å“ -->`

- **Section 3.3 - Performance Comparison**:
  - `<!-- AI_FILL: å¯¹æ¯”åˆ†ææ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†æˆ–ä¸åŸºå‡†æ¨¡å‹çš„æ€§èƒ½å·®å¼‚ -->`

- **Section 4.2 - Visualization Analysis** â† MOST CRITICAL:
  - `<!-- AI_FILL: åˆ†æå¯è§†åŒ–å›¾è¡¨å†…å®¹ï¼Œè¯´æ˜æ¯ä¸ªå›¾è¡¨å±•ç¤ºçš„ä¿¡æ¯å’Œå…³é”®å‘ç° -->`

- **Section 5 - Model Checkpoints**:
  - `<!-- AI_FILL: åˆ—å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„è¾…åŠ©æ–‡ä»¶ï¼Œå¦‚æ—¥å¿—ã€é…ç½®å¤‡ä»½ç­‰ -->`
  - `<!-- AI_FILL: æè¿°æ¨¡å‹é¢„æµ‹ç»“æœçš„è´¨é‡å’Œç‰¹ç‚¹ -->`

- **Section 6 - Training Analysis**:
  - `<!-- AI_FILL: åˆ†æè®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼ŒåŒ…æ‹¬ï¼šlossä¸‹é™è¶‹åŠ¿ã€æ˜¯å¦æœ‰å¼‚å¸¸æ³¢åŠ¨ã€æ”¶æ•›é€Ÿåº¦è¯„ä¼° -->`
  - `<!-- AI_FILL: åˆ†ææ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼šPSNR/SSIMç­‰æŒ‡æ ‡å˜åŒ–è¶‹åŠ¿ã€ä¸é¢„æœŸç›®æ ‡çš„å¯¹æ¯”ã€æ€§èƒ½ç“¶é¢ˆåˆ†æ -->`

- **Section 7 - Computational Performance**:
  - `<!-- AI_FILL: åˆ†æGPUåˆ©ç”¨ç‡æƒ…å†µï¼ŒåŒ…æ‹¬ï¼šæ˜¾å­˜å ç”¨ã€è®¡ç®—åˆ©ç”¨ç‡ã€æ˜¯å¦å­˜åœ¨ç“¶é¢ˆ -->`
  - `<!-- AI_FILL: åˆ†ææ•°æ®åŠ è½½æ•ˆç‡ï¼ŒåŒ…æ‹¬ï¼šæ•°æ®é¢„å¤„ç†æ—¶é—´ã€IOç“¶é¢ˆã€å»ºè®®ä¼˜åŒ–æ–¹å‘ -->`

- **Section 8 - Summary**:
  - `<!-- AI_FILL: æ€»ç»“æœ¬æ¬¡è®­ç»ƒçš„æ ¸å¿ƒæˆå°±ï¼ŒåŒ…æ‹¬ï¼šæ¨¡å‹æ€§èƒ½äº®ç‚¹ã€è®­ç»ƒæ•ˆç‡ã€è¾¾æˆçš„ç›®æ ‡ï¼ˆ3-5ç‚¹ï¼‰ -->`

### How to Fill AI_FILL Placeholders

When you read the generated report and encounter AI_FILL placeholders:

**DO**:
- âœ… **Use visualization images as evidence** (they are already embedded in the report)
- âœ… **Reference specific visual patterns** (e.g., "ä»loss_curve.pngå¯ä»¥çœ‹å‡ºï¼ŒæŸå¤±åœ¨ç¬¬50ä¸ªepochåè¶‹äºå¹³ç¨³")
- âœ… **Provide quantitative analysis** (e.g., "è®­ç»ƒæŸå¤±ä»0.5é™è‡³0.01ï¼Œä¸‹é™äº†98%")
- âœ… **Identify key findings** (e.g., "éªŒè¯é›†PSNRåœ¨ç¬¬80ä¸ªepochè¾¾åˆ°å³°å€¼åç•¥æœ‰ä¸‹é™ï¼Œæç¤ºå¯èƒ½å‡ºç°è½»å¾®è¿‡æ‹Ÿåˆ")
- âœ… **Give actionable insights** (e.g., "å»ºè®®åœ¨æœªæ¥è®­ç»ƒä¸­åœ¨ç¬¬80ä¸ªepochå¤„æ—©åœä»¥é¿å…è¿‡æ‹Ÿåˆ")

**DON'T**:
- âŒ Leave AI_FILL placeholders empty or unfilled
- âŒ Provide generic analysis without referencing visualizations
- âŒ Copy-paste metrics from tables without interpretation
- âŒ Skip analysis of critical visualizations (loss curves, metric trends)

### Supported Visualizations

**Geospatial/Geographic Plots**:
- `plot_type`: 'geospatial', 'map', 'scatter_map', 'contour_map', 'heatmap_map'
- Perfect for: SST maps, ocean data distribution, spatial analysis

**Standard Charts**:
- `plot_type`: 'line', 'scatter', 'bar', 'histogram', 'box', 'violin', 'pie', 'area', 'heatmap'
- Perfect for: Loss curves, metric comparison, data distributions

**Time Series**:
- `plot_type`: 'timeseries', 'forecast'
- Perfect for: Training history, temporal predictions

### Typical Post-Training Visualizations

**1. Loss Curves** (Always generate):
```json
{
  "data_source": "outputs/training_log.csv",
  "plot_type": "line",
  "output_path": "outputs/visualizations/loss_curve.png",
  "x_column": "epoch",
  "y_column": "train_loss,val_loss",
  "title": "Training and Validation Loss",
  "legend": true,
  "grid": true
}
```

**2. Performance Metrics** (PSNR, SSIM, etc.):
```json
{
  "data_source": "outputs/training_log.csv",
  "plot_type": "line",
  "output_path": "outputs/visualizations/psnr_curve.png",
  "x_column": "epoch",
  "y_column": "val_psnr",
  "title": "Validation PSNR Over Training",
  "color": "green",
  "grid": true
}
```

**3. Spatial Distribution Map** (For ocean data):
```json
{
  "data_source": "outputs/predictions.csv",
  "plot_type": "scatter_map",
  "output_path": "outputs/visualizations/sst_distribution.png",
  "longitude_column": "lon",
  "latitude_column": "lat",
  "value_column": "predicted_sst",
  "projection": "PlateCarree",
  "colormap": "coolwarm",
  "title": "Predicted Sea Surface Temperature"
}
```

**4. Error Distribution** (For model evaluation):
```json
{
  "data_source": "outputs/errors.csv",
  "plot_type": "histogram",
  "output_path": "outputs/visualizations/error_histogram.png",
  "x_column": "prediction_error",
  "bins": 50,
  "title": "Prediction Error Distribution"
}
```

### Complete Example Workflow

**Scenario**: User completes DiffSR training and wants a comprehensive report

```markdown
1. Training completes â†’ Files generated:
   - outputs/training_log.csv
   - outputs/metrics.json
   - outputs/config.json

2. Generate visualizations (YOU DO THIS - Step 2):

   a) Call OceanVisualization for loss curve
      â†’ Returns: "outputs/visualizations/loss_curve.png"

   b) Call OceanVisualization for PSNR curve
      â†’ Returns: "outputs/visualizations/psnr_curve.png"

   c) Call OceanVisualization for SSIM curve
      â†’ Returns: "outputs/visualizations/ssim_curve.png"

   d) Call OceanVisualization for spatial map
      â†’ Returns: "outputs/visualizations/sst_map.png"

   **Collect paths**: viz_paths = [
     "outputs/visualizations/loss_curve.png",
     "outputs/visualizations/psnr_curve.png",
     "outputs/visualizations/ssim_curve.png",
     "outputs/visualizations/sst_map.png"
   ]

3. Generate report with visualizations (Step 3):

   bash: python /opt/kode/dist/services/diffsr/report_generator.py train \
         outputs/config.json \
         outputs/metrics.json \
         outputs/training_report.md \
         --viz_paths "outputs/visualizations/loss_curve.png,outputs/visualizations/psnr_curve.png,outputs/visualizations/ssim_curve.png,outputs/visualizations/sst_map.png"

   â†’ Report generated: outputs/training_report.md
   â†’ VIZ_FILE_LIST and VIZ_IMAGES placeholders automatically filled
   â†’ AI_FILL placeholders preserved for manual analysis

4. AI analysis (YOU DO THIS - Step 4):

   a) Read the generated report:
      â†’ Use Read tool: outputs/training_report.md

   b) View embedded visualizations:
      â†’ Images are already embedded in the report via markdown

   c) Analyze each visualization:
      â†’ loss_curve.png: convergence pattern, stability, overfitting signs
      â†’ psnr_curve.png: performance evolution, peak epoch, trends
      â†’ ssim_curve.png: structural similarity trends, correlation with PSNR
      â†’ sst_map.png: spatial prediction quality, regional patterns

   d) Fill ALL AI_FILL placeholders:
      â†’ Use Edit tool to replace each AI_FILL comment with detailed analysis
      â†’ Reference specific visualizations in your analysis
      â†’ Provide quantitative insights from the data
      â†’ Give actionable recommendations

5. Present final report to user:
   - Report path: outputs/training_report.md
   - Visualizations embedded in Section 4
   - All VIZ placeholders filled automatically
   - All AI_FILL placeholders filled with your analysis
```

### File Naming Conventions

**Semantic Naming Standards** (for easier AI analysis):
- `loss_curve.png` - Training/validation loss trends
- `psnr_curve.png`, `ssim_curve.png` - Metric evolution curves
- `learning_rate_schedule.png` - Learning rate changes over epochs
- `sst_distribution.png` - Spatial SST map
- `error_histogram.png` - Error distribution histogram
- `metrics_comparison.png` - Bar chart comparing multiple metrics
- `prediction_vs_groundtruth.png` - Scatter plot of predictions vs actual values

**Naming Tips**:
- Use descriptive names that clearly indicate the content
- Use lowercase with underscores (snake_case)
- Include the metric/variable name in the filename
- Avoid generic names like "plot1.png" or "figure.png"

**Standard Directory**:
- Always save to `outputs/visualizations/` for consistency

### Common Mistakes to Avoid

1. âŒ **Generating report before visualizations**
   - âœ… Correct order: Visualize â†’ Collect paths â†’ Generate report â†’ Fill AI_FILL

2. âŒ **Forgetting to collect output paths**
   - âœ… Always save the output_path values to pass to report_generator.py

3. âŒ **Not reading CSV before plotting**
   - âœ… Always read the data file first to verify column names (use Read tool)

4. âŒ **Skipping visualizations to save time**
   - âœ… Visualizations are critical for understanding model performance AND for AI analysis

5. âŒ **Using inconsistent file paths**
   - âœ… Always use "outputs/visualizations/" as the base directory

6. âŒ **Using generic filenames**
   - âœ… Use semantic, descriptive names (loss_curve.png, not plot1.png)

7. âŒ **Stopping after report generation** â† MOST CRITICAL ERROR
   - âœ… **You MUST read the report and fill ALL AI_FILL placeholders**
   - âœ… This is NOT optional - it's a required step in the workflow

8. âŒ **Filling AI_FILL placeholders without viewing visualizations**
   - âœ… **Always use visual evidence in your analysis**
   - âœ… Reference specific visualization files in your analysis

9. âŒ **Leaving AI_FILL placeholders empty**
   - âœ… Fill ALL AI_FILL placeholders with detailed, data-driven analysis

10. âŒ **Providing generic analysis without specifics**
    - âœ… Provide quantitative observations, identify patterns, give actionable insights

Following this policy ensures all visualizations are properly generated, integrated with reports, maintain professional quality, AND are properly analyzed by AI.

